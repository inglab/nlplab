{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import reuters\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def word_tokenizer(text):\n",
    "        #tokenizes and stems the text\n",
    "        tokens = word_tokenize(text)\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens if t not in stopwords.words('english')]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def vectorize(sentences):\n",
    "        tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenizer,\n",
    "                                        stop_words=stopwords.words('english'),\n",
    "                                        max_df=0.9,\n",
    "                                        min_df=0.1,\n",
    "                                        lowercase=True)\n",
    "        #builds a tf-idf matrix for the sentences\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "        return tfidf_matrix\n",
    "\n",
    "    \n",
    "def k_means_cluster(sentences, nb_of_clusters=3, initialize='random'):\n",
    "        kmeans = KMeans(n_clusters=nb_of_clusters, init=initialize, n_init=1, )\n",
    "        tfidf_matrix = vectorize(sentences)\n",
    "        tfidf_array = tfidf_matrix.toarray()\n",
    "        kmeans.fit(tfidf_array)\n",
    "        clusters = collections.defaultdict(list)\n",
    "        for i, label in enumerate(kmeans.labels_):\n",
    "                clusters[label].append(i)\n",
    "        return dict(clusters)\n",
    "\n",
    "\n",
    "def euc_distance(a, b):\n",
    "    total = 0\n",
    "    if len(a) == len(b):\n",
    "        for i in range(len(a)):\n",
    "            total += math.pow((a[i] - b[i]), 2)\n",
    "        return math.sqrt(total)\n",
    "\n",
    "    else:\n",
    "        print(\"a, b not same length\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_distance(array, centers): # return(distance, index of cluster)\n",
    "    distances=[]\n",
    "    for center_id in range(len(centers)):\n",
    "        dist = euc_distance(array, centers[center_id])\n",
    "        distances.append((dist, center_id))\n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "def find_center(tfidf_array, _cluster_id, num_of_clusters):\n",
    "    cluster_count = np.zeros(num_of_clusters)\n",
    "    new_centers = np.array([np.zeros(len(tfidf_array[0])) for i in range(num_of_clusters)])\n",
    "\n",
    "    for i in range(len(tfidf_array)):\n",
    "        id  = int(_cluster_id[i])\n",
    "        cluster_count[id] += 1\n",
    "        new_centers[id] = new_centers[id] + tfidf_array[i]\n",
    "\n",
    "    for i in range(num_of_clusters):\n",
    "        if cluster_count[i] != 0:\n",
    "            new_centers[i] = new_centers[i] / cluster_count[i]\n",
    "\n",
    "    return new_centers\n",
    "\n",
    "\n",
    "\n",
    "def find_cluster(tfidf_array, centers):\n",
    "    new_cluster_ = np.zeros(len(tfidf_array))  # [0 for i in range(len(tfidf_array))]\n",
    "    for i in range(len(tfidf_array)):\n",
    "        distances = get_distance(tfidf_array[i], centers)  # return (distance, center_id)\n",
    "        distances = sorted(distances, key=lambda x: x[0])\n",
    "        cluster_id = distances[0][1]\n",
    "        new_cluster_[i] = cluster_id\n",
    "    return new_cluster_\n",
    "\n",
    "\n",
    "\n",
    "def my_k_means_cluster(sentences, num_of_clusters, initialize):\n",
    "\n",
    "    # vectorize\n",
    "    tfidf_matrix = vectorize(sentences)\n",
    "    tfidf_array = tfidf_matrix.toarray()\n",
    "    num_of_data = len(tfidf_array)\n",
    "\n",
    "    # initialize center\n",
    "    if (type(initialize) == str ) and (initialize == 'random'):\n",
    "        cluster_center_index = random.sample(range(num_of_data), num_of_clusters)\n",
    "        cluster_center = [tfidf_array[idx] for idx in cluster_center_index]\n",
    "    else:\n",
    "        cluster_center = initialize # center 고정\n",
    "\n",
    "    # cluster init to -1\n",
    "    cluster_id = np.zeros(num_of_data) - 1\n",
    "    # find new cluster\n",
    "    new_cluster_id = find_cluster(tfidf_array, cluster_center)\n",
    "\n",
    "    # find new center and new cluster until convergence\n",
    "    while not np.array_equal(cluster_id, new_cluster_id):\n",
    "        cluster_center = find_center(tfidf_array, new_cluster_id, num_of_clusters)\n",
    "        cluster_id = new_cluster_id.copy()\n",
    "        new_cluster_id = find_cluster(tfidf_array, cluster_center)\n",
    "\n",
    "    return new_cluster_id  #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM INIT\n",
    "\n",
    "files = reuters.fileids('coffee')\n",
    "coffee0 = reuters.raw(fileids=files[0])\n",
    "coffee1 = reuters.raw(fileids=files[1])\n",
    "coffee2 = reuters.raw(fileids=files[2])\n",
    "coffee3 = reuters.raw(fileids=files[3])\n",
    "\n",
    "files = reuters.fileids('cotton')\n",
    "cotton0 = reuters.raw(fileids=files[0])\n",
    "cotton1 = reuters.raw(fileids=files[1])\n",
    "cotton2 = reuters.raw(fileids=files[2])\n",
    "cotton3 = reuters.raw(fileids=files[3])\n",
    "\n",
    "files = reuters.fileids('crude')\n",
    "crude0 = reuters.raw(fileids=files[0])\n",
    "crude1 = reuters.raw(fileids=files[1])\n",
    "crude2 = reuters.raw(fileids=files[2])\n",
    "crude3 = reuters.raw(fileids=files[3])\n",
    "\n",
    "sentences = [coffee0, coffee1, coffee2, cotton0, cotton1, cotton2, crude0, crude1, crude2]\n",
    "nclusters= 3\n",
    "init= 'random'\n",
    "\n",
    "my_clusters = my_k_means_cluster(sentences, nclusters, init)\n",
    "print(\"my_kmeans\", my_clusters)\n",
    "\n",
    "sklearn_clusters = k_means_cluster(sentences, nclusters, init)\n",
    "print(\"sklearn\", sklearn_clusters)\n",
    "\n",
    "\n",
    "#\n",
    "# for cluster in range(nclusters):\n",
    "#     print (\"cluster \",cluster,\":\")\n",
    "#     for i,sentence in enumerate(sklearn_clusters[cluster]):\n",
    "#         print (\"\\tsentence \",sentence,\": \",sentences[sentence][:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = reuters.fileids('coffee')\n",
    "coffee0 = reuters.raw(fileids=files[0])\n",
    "coffee1 = reuters.raw(fileids=files[1])\n",
    "coffee2 = reuters.raw(fileids=files[2])\n",
    "coffee3 = reuters.raw(fileids=files[3])\n",
    "\n",
    "files = reuters.fileids('cotton')\n",
    "cotton0 = reuters.raw(fileids=files[0])\n",
    "cotton1 = reuters.raw(fileids=files[1])\n",
    "cotton2 = reuters.raw(fileids=files[2])\n",
    "cotton3 = reuters.raw(fileids=files[3])\n",
    "\n",
    "files = reuters.fileids('crude')\n",
    "crude0 = reuters.raw(fileids=files[0])\n",
    "crude1 = reuters.raw(fileids=files[1])\n",
    "crude2 = reuters.raw(fileids=files[2])\n",
    "crude3 = reuters.raw(fileids=files[3])\n",
    "\n",
    "sentences = [coffee0, coffee1, coffee2, cotton0, cotton1, cotton2, crude0, crude1, crude2]\n",
    "nclusters= 3\n",
    "\n",
    "## CENTER NOT RANDOM \n",
    "tfidf_matrix = vectorize(sentences)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "init = np.array([tfidf_array[2], tfidf_array[4], tfidf_array[8]])\n",
    "\n",
    "\n",
    "my_clusters = my_k_means_cluster(sentences, nclusters, init)\n",
    "print(\"my_kmeans\", my_clusters)\n",
    "\n",
    "sklearn_clusters = k_means_cluster(sentences, nclusters, init)\n",
    "print(\"sklearn\", sklearn_clusters)\n",
    "\n",
    "\n",
    "#\n",
    "# for cluster in range(nclusters):\n",
    "#     print (\"cluster \",cluster,\":\")\n",
    "#     for i,sentence in enumerate(sklearn_clusters[cluster]):\n",
    "#         print (\"\\tsentence \",sentence,\": \",sentences[sentence][:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
