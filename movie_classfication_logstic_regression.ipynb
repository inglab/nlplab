{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.958832025527954\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석 후 txt 로 저장\n",
    "make_tokenized_text = 0 \n",
    "if make_tokenized_text:\n",
    "    import time\n",
    "    s = time.time()\n",
    "    from konlpy.tag import Twitter\n",
    "    twitter = Twitter()\n",
    "\n",
    "    rawtext='merged_comments.txt'\n",
    "    outputtext = 'merged_comments_tokenized.txt'\n",
    "    with open(rawtext,encoding='utf8') as f:\n",
    "        with open(outputtext, 'w', encoding='utf8') as output:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                line=line.split('\\t')\n",
    "                sentence = line[1]\n",
    "                score = line[2]\n",
    "                twit = twitter.pos(sentence, stem=True, norm=True)\n",
    "    #             sentence = [word + \"/\" + pos[0] + pos[-1] for (word, pos) in twit if pos != 'Josa']\n",
    "                sentence = [word for (word, pos) in twit if pos != 'Josa']\n",
    "                outputline = \" \".join(sentence) + \"\\t\" + score\n",
    "    #             print(outputline)\n",
    "                output.writelines(outputline)\n",
    "                line=f.readline()\n",
    "    e = time.time()\n",
    "    print( (e-s)/60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45290\t크리스토퍼 놀란 에게 우리는 놀란 다\t10\n",
      "45290\t인셉션 정말 흥미진진하게 봤었고 크리스토퍼 놀란 감독님 신작 인터스텔라도 이번주 일요일에 보러갑니다 완전 기대중\t10\n",
      "45290\t놀란이면 무조건 봐야 된다 왜냐하면 모든 작품을 다 히트 쳤으니깐\t10\n",
      "45290\t나는 감탄할 준비가 되어있다\t10\n",
      "45290\t얘들아 오늘나오는거지 밤에 ㅋㅋ 오늘 보러가야겟다\t10\n",
      "45290\t이제 죽어도 여한이 없다\t10\n",
      "45290\t기대감에 잠도 안온다\t10\n",
      "45290\t나랑 같이 봐 줄까 ㅎ\t10\n",
      "45290\t우선 명감독들이 모였고 미국에서도 극찬을 받았더군\t10\n",
      "45290\t드디어 내일 꼭본다\t10\n"
     ]
    }
   ],
   "source": [
    "#데이터 체크 \n",
    "rawtext= 'merged_comments.txt'\n",
    "with open(rawtext, encoding= 'utf8')as f:\n",
    "    for _ in range(10):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294493\n"
     ]
    }
   ],
   "source": [
    "def load_reviews(fname, n_limit=-1): \n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        idxs = []\n",
    "        scores = []\n",
    "        texts = []\n",
    "        \n",
    "        for i, doc in enumerate(f):\n",
    "            if n_limit > 0 and i >= n_limit:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                idx, text, score = doc.strip().split('\\t')\n",
    "                idxs.append(idx)\n",
    "                texts.append(text.strip())\n",
    "                scores.append(int(score))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(doc)\n",
    "                continue\n",
    "                \n",
    "    return idx, texts, scores\n",
    "\n",
    "idx, texts, scores = load_reviews(rawtext)\n",
    "print(len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('영화', 40000),\n",
      " ('정말', 18266),\n",
      " ('진짜', 14207),\n",
      " ('너무', 13397),\n",
      " ('이', 7893),\n",
      " ('영화를', 7006),\n",
      " ('그냥', 6844),\n",
      " ('더', 6560),\n",
      " ('최고의', 6394),\n",
      " ('보고', 5899),\n",
      " ('좀', 5725),\n",
      " ('수', 5671),\n",
      " ('영화가', 5588),\n",
      " ('최고', 5347),\n",
      " ('영화는', 5308),\n",
      " ('잘', 5039),\n",
      " ('꼭', 4944),\n",
      " ('ㅋㅋ', 4865),\n",
      " ('본', 4655),\n",
      " ('다', 4609)]\n",
      "limit = 1, the number of words : 400649\n",
      "limit = 2, the number of words : 100035\n",
      "limit = 3, the number of words : 61982\n",
      "limit = 4, the number of words : 46182\n",
      "limit = 5, the number of words : 37222\n",
      "limit = 6, the number of words : 31295\n",
      "limit = 7, the number of words : 27206\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter( [word for text in texts for word in text.strip().split()] )\n",
    "\n",
    "# 빈도수 상위 20개 단어 출력\n",
    "from pprint import pprint\n",
    "top = sorted(word_count.items(), key=lambda x: x[1], reverse =True)[:20]\n",
    "pprint(top)\n",
    "\n",
    "\n",
    "\n",
    "# n 번 이상 출현한 단어들\n",
    "for limit in [1,2,3,4,5,6,7]:\n",
    "    word_count_limit={ word for word,frequency in word_count.items()\n",
    "                     if frequency >=limit}\n",
    "    print('limit = %d, the number of words : %d'%(limit, len(word_count_limit)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['영화가',\n",
      " '이렇게',\n",
      " '이',\n",
      " '그',\n",
      " '본',\n",
      " '영화',\n",
      " '이런',\n",
      " '영화는',\n",
      " '영화를',\n",
      " '봤는데',\n",
      " '정말',\n",
      " '완전',\n",
      " '왜',\n",
      " '평점',\n",
      " '내',\n",
      " '보고',\n",
      " '난',\n",
      " '다',\n",
      " '내가',\n",
      " '너무',\n",
      " '이건',\n",
      " 'ㅋㅋ',\n",
      " '더',\n",
      " '좀',\n",
      " '그냥',\n",
      " '진짜',\n",
      " 'ㅋ']\n"
     ]
    }
   ],
   "source": [
    "# stopwords\n",
    "possitive_limit=9\n",
    "negative_limit=3\n",
    "\n",
    "possitive_word_count = Counter( [word for (text,score) in zip(texts,scores) for word in text.strip().split() if score >= possitive_limit ]  )\n",
    "negative_word_count = Counter( [word for (text,score) in zip(texts,scores) for word in text.strip().split() if score <= negative_limit ]  )\n",
    "\n",
    "top_possitive = [word for (word,count) in (sorted(possitive_word_count.items(), key =lambda x:x[1], reverse= True)[:50]) ]\n",
    "top_negative=[word for (word,count) in sorted(negative_word_count.items(), key = lambda x:x[1], reverse = True)[:50]]\n",
    "\n",
    "stopwords = list(set(top_possitive) & set(top_negative))\n",
    "pprint(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19737\n",
      "놀란이면 무조건 봐야 된다 왜냐하면 모든 작품을 다 히트 쳤으니깐\n",
      "['무조건', '봐야', '된다', '왜냐하면', '모든', '작품을']\n"
     ]
    }
   ],
   "source": [
    "# 빈도 10 이상인 단어들만 남기기 + stopwords 제거\n",
    "\n",
    "word_count_limit = { word for word, frequency in word_count.items()\n",
    "                   if frequency >=10 and word not in stopwords}\n",
    "\n",
    "\n",
    "def my_word_tokenizer(sentence_in_string):\n",
    "    return [word for word in sentence_in_string.strip().split()\n",
    "            if word in word_count_limit]\n",
    "\n",
    "print(len(word_count_limit))\n",
    "print(texts[2])\n",
    "print(my_word_tokenizer(texts[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 1: (56122, 19.057 perc)\n",
      "score = 2: (4725, 1.604 perc)\n",
      "score = 3: (4547, 1.544 perc)\n",
      "score = 4: (4062, 1.379 perc)\n",
      "score = 5: (7697, 2.614 perc)\n",
      "score = 6: (7588, 2.577 perc)\n",
      "score = 7: (11338, 3.850 perc)\n",
      "score = 8: (20311, 6.897 perc)\n",
      "score = 9: (25528, 8.668 perc)\n",
      "score = 10: (152575, 51.809 perc)\n"
     ]
    }
   ],
   "source": [
    "#영화 평점 별 리뷰 수\n",
    "from collections import Counter\n",
    "for score, score_freq in sorted(Counter(scores).items()):\n",
    "    print('score = %d: (%d, %.3f perc)' % (score, score_freq, 100*score_freq/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 294493 --> 221252\n",
      "label = 1: (162846, 73.602 perc)\n",
      "label = -1: (58406, 26.398 perc)\n"
     ]
    }
   ],
   "source": [
    "# 1~3 점은 -1\n",
    "# 9,10점은 1\n",
    "# 나머지는 사용하지 않음\n",
    "\n",
    "train_texts = []\n",
    "train_label = []\n",
    "\n",
    "\n",
    "for review, score in zip(texts, scores):\n",
    "    if negative_limit <  score <  possitive_limit:\n",
    "        continue\n",
    "    tokenized_review = my_word_tokenizer(review)\n",
    "#     words = [word for word in review.split() if word in word_dictionary]\n",
    "    if not tokenized_review:\n",
    "        continue\n",
    "  \n",
    "    train_texts.append(tokenized_review)\n",
    "    train_label.append(1 if score >= possitive_limit else -1)\n",
    "\n",
    "print('train data: %d --> %d' % (len(texts), len(train_texts)))\n",
    "for label, label_freq in Counter(train_label).items():\n",
    "    print('label = %d: (%d, %.3f perc)' % (label, label_freq, 100*label_freq/len(train_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<221252x19735 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1040849 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 빈도수로 벡터화\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#train_texts가 string 이라면 tokenizer 설정 필요 x\n",
    "vectorizer= CountVectorizer(\n",
    "    tokenizer = lambda x:x , lowercase = False, \n",
    "  \n",
    "    )\n",
    "\n",
    "train_x= vectorizer.fit_transform(train_texts)\n",
    "print(type(train_x) ) \n",
    "train_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15378\n",
      "재미\n"
     ]
    }
   ],
   "source": [
    "# 특정 단어에 해당하는 인덱스 알아보기\n",
    "\n",
    "vocab2int = vectorizer.vocabulary_ #dictionary, {word: index} 형태\n",
    "print(vocab2int['재미'])\n",
    "\n",
    "int2vocab = [word for word,index in sorted # 단어를 철자순으로 정렬한 list\n",
    "             (vocab2int.items(), key=lambda x:x[1])] \n",
    "\n",
    "\n",
    "print(int2vocab[15378])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(train_x, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['재미없다', '이상', '10자']\n",
      "\n",
      "predicted class prob: (negative= 0.740, positive= 0.260\n",
      "predicted class = negative\n",
      "actual class = negative\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "text: ['대박나세요']\n",
      "\n",
      "predicted class prob: (negative= 0.123, positive= 0.877\n",
      "predicted class = positive\n",
      "actual class = positive\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "text: ['쿠퍼가', '장면이', '잊혀지질', '않네요']\n",
      "\n",
      "predicted class prob: (negative= 0.124, positive= 0.876\n",
      "predicted class = positive\n",
      "actual class = positive\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "text: ['감독', '이름', '조심해라']\n",
      "\n",
      "predicted class prob: (negative= 0.859, positive= 0.141\n",
      "predicted class = negative\n",
      "actual class = negative\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "text: ['나', '혼자만', 'ㄷㄷ']\n",
      "\n",
      "predicted class prob: (negative= 0.345, positive= 0.655\n",
      "predicted class = positive\n",
      "actual class = positive\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n 번째 리뷰를 골라 predict 해보기\n",
    "\n",
    "for idx in [14,100,1000,199999,200006]:\n",
    "    print('text: %s\\n' % (train_texts[idx]))\n",
    "    \n",
    "    prob = logistic.predict_proba(train_x[idx])[0]\n",
    "    pred_class = logistic.predict(train_x[idx])\n",
    "    actual_class = train_label[idx]\n",
    "    \n",
    "    if actual_class ==1:\n",
    "        actual_class=\"positive\"\n",
    "    else:\n",
    "        actual_class=\"negative\"\n",
    "    \n",
    "    print('predicted class prob: (negative= %.3f, positive= %.3f' \n",
    "          % tuple(prob))\n",
    "    print('predicted class = %s' %('positive') if pred_class == 1 \n",
    "          else 'predicted class = %s'%('negative'))\n",
    "    print('actual class = %s'%(actual_class))\n",
    "    print('\\n%s\\n' % ('-'*50))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "# 사용자로부터 새로운 인풋을 받아서 predict 하고 싶을 때\n",
    "\n",
    "# vectorizer_ = vectorizer.fit(train_texts)\n",
    "# new_input = vectorizer_.transform( [\"와 정말 재미있는 영화네요 이건 긍정적인 문장일까요?\".split()] )\n",
    "# new_pred_class = logistic.predict(new_input)\n",
    "# if new_pred_class == 1:\n",
    "#     print(\"positive\")\n",
    "# else:\n",
    "#     print(\"negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "알이즈웰 (4.322)\n",
      "최고입니다 (3.525)\n",
      "기대됩니다 (3.236)\n",
      "관람객재밌어요 (3.219)\n",
      "10점준다 (3.160)\n",
      "좋았어요 (3.039)\n",
      "웰 (3.022)\n",
      "재미있었습니다 (3.016)\n",
      "꿀잼 (2.962)\n",
      "인생영화 (2.928)\n"
     ]
    }
   ],
   "source": [
    "#coefficients 살펴보기\n",
    "\n",
    "# 긍정리뷰로 예측하는데 영향을 미친 단어들 상위 10\n",
    "coefficients = logistic.coef_.tolist()\n",
    "sorted_coefficients = sorted(enumerate(coefficients[0]), \n",
    "                             key=lambda x:x[1], reverse=True)\n",
    "\n",
    "for index, coef in sorted_coefficients[:10]:\n",
    "    print('%s (%.3f)' % (int2vocab[index], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0점은 (-4.377)\n",
      "최악 (-4.216)\n",
      "최악의 (-4.191)\n",
      "0점이 (-3.974)\n",
      "1점준다 (-3.837)\n",
      "돈아깝다 (-3.749)\n",
      "쓰레기영화 (-3.662)\n",
      "노잼 (-3.570)\n",
      "1점대 (-3.544)\n",
      "최악의영화 (-3.491)\n"
     ]
    }
   ],
   "source": [
    "# 부정리뷰로 예측하는데 영향을 미친 단어들 상위 10\n",
    "# coefficients = logistic.coef_.tolist()\n",
    "sorted_coefficients = sorted(enumerate(coefficients[0]), \n",
    "                             key=lambda x:x[1])\n",
    "\n",
    "\n",
    "for index, coef in sorted_coefficients[:10]:\n",
    "\n",
    "    print('%s (%.3f)' % (int2vocab[index], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[150112  12734]\n",
      " [ 20842  37564]]\n",
      " accuacy: 0.848\n",
      " recall : 0.922\n",
      " precision: 0.878\n"
     ]
    }
   ],
   "source": [
    "# 5 - FOLD CORSSVAL \n",
    "\n",
    "y_predicted = cross_val_predict(logistic,train_x,train_label,cv=5)\n",
    "conf_mat = confusion_matrix(train_label,y_predicted, labels=[1, -1])\n",
    "TP, FN, FP, TN =conf_mat[0][0], conf_mat[0][1], conf_mat[1][0], conf_mat[1][1]\n",
    "\n",
    "print(conf_mat)\n",
    "print(\" accuacy: %.3f\\n recall : %.3f\\n precision: %.3f\"%( (TP+TN)/(TP+TN+FN+FP), TP/(TP+FN), TP/(TP+FP)  ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
